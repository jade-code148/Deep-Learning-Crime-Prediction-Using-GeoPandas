# -*- coding: utf-8 -*-
"""crime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rLeZUEb1I7LmxNnZ8W5230jzW1g_CIbA
"""

pip install numpy pandas matplotlib scikit-learn tensorflow

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM

# Load the dataset
file_path = '/content/crime.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the dataset
print(data.head())

print(data.shape)

# Check the raw data before transforming
print(data.head())
print(data.info())

data.head()

df_long = data.melt(id_vars=['STATE/UT', 'CRIME HEAD'], var_name='year', value_name='cases')
# 'data' is the variable you defined when reading the csv file

# Pivot data
df_pivot = df_long.pivot_table(index=['STATE/UT', 'CRIME HEAD'], columns='year', values='cases')

# Fill missing values if any
df_pivot.fillna(0, inplace=True)

print(df_pivot.head())
print(df_pivot.shape)

df_reset = df_pivot.reset_index()

df_long = df_reset.melt(id_vars=['STATE/UT', 'CRIME HEAD'], var_name='year', value_name='cases')
df_long['year'] = df_long['year'].astype(int)

def create_sequences(df, sequence_length):
    X, y, y_true = [], [], []

    for state in df['STATE/UT'].unique():
        for crime in df['CRIME HEAD'].unique():
            subset = df[(df['STATE/UT'] == state) & (df['CRIME HEAD'] == crime)]

            # Pivot to get years as rows and arrests as values
            values = subset.pivot(index='year', columns='CRIME HEAD', values='cases').values

            # Create sequences
            for i in range(len(values) - sequence_length):
                X.append(values[i:i+sequence_length])
                y.append(values[i+sequence_length])
                y_true.append(values[i+sequence_length])  # Save actual values

    return np.array(X), np.array(y), np.array(y_true)

# Create sequences with a specified length
sequence_length = 5
X, y, y_true = create_sequences(df_long, sequence_length)

scaler_X = MinMaxScaler(feature_range=(0, 1))
X_reshaped = X.reshape(-1, X.shape[2])
X_scaled = scaler_X.fit_transform(X_reshaped).reshape(X.shape)

# Initialize and fit scaler for y
scaler_y = MinMaxScaler(feature_range=(0, 1))
y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).reshape(y.shape)

model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(sequence_length, X.shape[2])))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

print(model.summary())

# Train the model
history = model.fit(X_scaled, y_scaled, epochs=10, batch_size=32, validation_split=0.2)

# Predict with the model
y_pred_scaled = model.predict(X_scaled)

# Output actual and predicted values
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(y_scaled.shape) # Use scaler_y to inverse transform
y_true_original = scaler_y.inverse_transform(y_true.reshape(-1, 1)).reshape(y_true.shape) # Use scaler_y to inverse transform

comparison_df = pd.DataFrame({
    'Actual': y_true_original.flatten(),
    'Predicted': y_pred.flatten()
})

print(comparison_df.head())

def predict_future(model, last_sequence, scaler_y, num_steps=3):
    predictions = []
    sequence = last_sequence

    for _ in range(num_steps):
        y_pred_scaled = model.predict(sequence)
        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).reshape(1, -1)
        predictions.append(y_pred.flatten())

        # Update sequence with the new prediction
        sequence = np.roll(sequence, shift=-1, axis=1)
        sequence[0, -1, 0] = y_pred.flatten()[0]

    return predictions

# Predict future values
last_sequence = X[-1].reshape((1, sequence_length, X.shape[2]))
future_predictions = predict_future(model, last_sequence, scaler_y, num_steps=3)

# Flatten the list of arrays and handle potential issues
future_predictions_flat = np.concatenate(future_predictions) if future_predictions else []

# Define crime_heads, num_crime_heads, and num_years before using them
crime_heads = df_long['CRIME HEAD'].unique()  # Assuming df_long is the DataFrame containing crime data
num_crime_heads = len(crime_heads)
num_years = 3  # Adjust as needed based on your prediction horizon

# Debug print statements
print(f"Crime Heads: {crime_heads}")
print(f"Number of Crime Heads: {num_crime_heads}")
print(f"Number of Years: {num_years}")
print(f"Future Predictions Length: {len(future_predictions_flat)}")

# ... (Rest of the code remains unchanged) ...

# Check if lengths match
expected_length = num_crime_heads * num_years
print(f"Expected Length: {expected_length}")

if len(future_predictions_flat) == expected_length:
    predictions_df = pd.DataFrame({
        'Crime Head': np.tile(crime_heads, num_years),
        'Year': np.repeat(range(2013, 2013 + num_years), num_crime_heads),
        'Predicted Arrests': future_predictions_flat
    })
else:
    #print("Mismatch in lengths. Adjusting data.")
    # Adjust predictions to fit the expected length
    adjusted_predictions = np.resize(future_predictions_flat, expected_length)

    predictions_df = pd.DataFrame({
        'Crime Head': np.tile(crime_heads, num_years),
        'Year': np.repeat(range(2013, 2013 + num_years), num_crime_heads),
        'Predicted Arrests': adjusted_predictions
    })

print(predictions_df)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

mse = mean_squared_error(y_true_original, y_pred)
mae = mean_absolute_error(y_true_original, y_pred)
r2 = r2_score(y_true_original, y_pred)

# Output metrics
print(f"Mean Squared Error (MSE): {mse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (R2): {r2}")

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import pandas as pd
import numpy as np

# Assuming df_pivot contains the crime data with states and crime types
# Normalize the data
scaler = StandardScaler()
data_normalized = scaler.fit_transform(df_pivot)

# Apply K-Means clustering
num_clusters = 5  # You can adjust this number
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
clusters = kmeans.fit_predict(data_normalized)

# Add cluster labels to the original DataFrame
df_pivot['Cluster'] = clusters

print(df_pivot.head())

import pandas as pd

# List of state names (example)
states = [
    'State1', 'State2', 'State3', 'State4', 'State5', 'State6',
    'State7', 'State8', 'State9', 'State10', 'State11', 'State12', 'State13'
]

# Example cluster data (make sure it's the same length as the states list)
# If you have fewer clusters than states, repeat the cluster list to match
clusters = [0, 1, 2]  # Example clusters
num_states = len(states)
num_clusters = len(clusters)

# Repeat clusters to match the length of states
repeated_clusters = clusters * (num_states // num_clusters) + clusters[:num_states % num_clusters]

# Check lengths
print(f"Length of states list: {len(states)}")
print(f"Length of repeated clusters list: {len(repeated_clusters)}")

# Create DataFrame
state_cluster_data = pd.DataFrame({
    'STATE/UT': states,
    'Cluster': repeated_clusters
})

print(state_cluster_data)

# Correct state names corresponding to the clusters
actual_state_names = [
    'Andaman and Nicobar', 'Andhra Pradesh', 'Arunachal Pradesh', 'Assam',
    'Bihar', 'Chandigarh', 'Chhattisgarh', 'Dadra and Nagar Haveli',
    'Daman and Diu', 'Delhi', 'Goa', 'Gujarat', 'Haryana'
]

# Update state names in state_cluster_data
state_cluster_data['STATE/UT'] = actual_state_names

# Check the updated DataFrame
print(state_cluster_data)

# Define the list of crime heads
crime_head_values = [
    'Total Crimes Against Children', 'Abetment of Suicide', 'Buying of Girls for Prostitution',
    'Exposure and Abandonment', 'Foeticide', 'Infanticide', 'Kidnapping and Abduction of Children',
    'Murder of Children', 'Other Crimes Against Children', 'Procurement of Minor Girls',
    'Prohibition of Child Marriage Act', 'Rape of Children', 'Selling of Girls for Prostitution'
]

# Ensure this list matches the length and order of your states

import pandas as pd

# Define lists
states = ['State1', 'State2', 'State3', 'State4', 'State5', 'State6', 'State7', 'State8',
          'State9', 'State10', 'State11', 'State12', 'State13']  # Length 13

clusters = [0, 1, 2]  # Length 3
crime_head_values = [
    'Total Crimes Against Children', 'Abetment of Suicide', 'Buying of Girls for Prostitution',
    'Exposure and Abandonment', 'Foeticide', 'Infanticide', 'Kidnapping and Abduction of Children',
    'Murder of Children', 'Other Crimes Against Children', 'Procurement of Minor Girls',
    'Prohibition of Child Marriage Act', 'Rape of Children', 'Selling of Girls for Prostitution'
]  # Length 13

# Adjust clusters to match the length of states
num_states = len(states)
num_clusters = len(clusters)

# Repeat clusters to match length
repeated_clusters = clusters * (num_states // num_clusters) + clusters[:num_states % num_clusters]

# Verify lengths
print(f"Length of states: {len(states)}")
print(f"Length of repeated_clusters: {len(repeated_clusters)}")
print(f"Length of crime_head_values: {len(crime_head_values)}")

# Create DataFrame
state_cluster_data = pd.DataFrame({
    'STATE/UT': states,
    'Cluster': repeated_clusters,
    'Crime Head': crime_head_values
})

print(state_cluster_data)

clusters = clusters * (len(states) // len(clusters)) + clusters[:len(states) % len(clusters)]

# Example mapping from your state names to the ones used in india_map
name_mapping = {
    'State1': 'Andaman and Nicobar',
    'State2': 'Andhra Pradesh',
    'State3': 'Arunachal Pradesh',
    'State4': 'Assam',
    'State5': 'Bihar',
    'State6': 'Chandigarh',
    'State7': 'Chhattisgarh',
    'State8': 'Dadra and Nagar Haveli',
    'State9': 'Daman and Diu',
    'State10': 'Delhi',
    'State11': 'Goa',
    'State12': 'Gujarat',
    'State13': 'Haryana'
    # Add mappings for all states as needed
}

state_cluster_data['STATE/UT'] = state_cluster_data['STATE/UT'].map(name_mapping)

print(state_cluster_data['STATE/UT'].isna().sum())

import geopandas as gpd

# Load the map data from a file
india_map = gpd.read_file('/content/india_state_geo.json')

# Now you can proceed with the merge operation
df_map = india_map.merge(state_cluster_data, left_on='NAME_1', right_on='STATE/UT', how='left')

# Check the result of the merge
print(df_map.head())
print(df_map.info())

import geopandas as gpd
import matplotlib.pyplot as plt

# Plot the map
fig, ax = plt.subplots(1, 1, figsize=(12, 12))

# Plot the entire outline of India
india_map.boundary.plot(ax=ax, linewidth=1.5, color='black')

# Overlay the states with clusters
plot = df_map.plot(column='Cluster', ax=ax, cmap='viridis', edgecolor='k', legend=False)

# Setting the x and y axis limits to ensure the entire map is visible
ax.set_xlim(india_map.total_bounds[0], india_map.total_bounds[2])
ax.set_ylim(india_map.total_bounds[1], india_map.total_bounds[3])

# Adding axis labels
ax.set_xlabel('Longitude', fontsize=14)
ax.set_ylabel('Latitude', fontsize=14)

# Adding a title
ax.set_title('Crime Clusters by State in India', fontsize=16, fontweight='bold')

# Customizing the legend manually
cbar = plot.get_figure().colorbar(plot.collections[0], ax=ax)
cbar.set_label('Cluster')
cbar.ax.set_ylabel('Cluster', fontsize=12)
cbar.ax.tick_params(labelsize=10)

# Add text annotations for crime heads
for idx, row in df_map.dropna(subset=['Cluster']).iterrows():
    if row.geometry.is_valid:
        ax.text(row.geometry.centroid.x, row.geometry.centroid.y,
                f"{row['Crime Head']}",
                fontsize=8, ha='center', color='black', bbox=dict(facecolor='white', alpha=0.7, edgecolor='black'))

plt.show()